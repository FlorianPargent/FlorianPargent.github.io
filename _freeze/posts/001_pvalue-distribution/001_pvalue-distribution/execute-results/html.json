{
  "hash": "7df7cfc8d3cadefbda0cb7ee62162640",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"P-value distribution under H0\"\ndescription: \"Some basic visualizations to build intuition on why the p-value is uniformly distributed if the null hypothesis is true.\"\ndate: 2024-05-19\ncategories: [\"p-value\", \"statistical literacy\", \"teaching\"]\n---\n\n\n:::{.callout-tip}\n# In this post\n\nOur students asked us whether there is any good intuition on why the p-value of a hypothesis test has a uniform distribution if the null hypothesis is true.\nSo I hacked up some basic visualizations.\n\n:::\n\n## Convince ourselves that the p-value is uniformly distributed under the null hypothesis\n\nFirst lets think about the quickest way to convince ourselves that the p-value is in fact uniformly distributed:\n\n1. Assume we have a test statistic, that is t-distributed under $H_0$. So lets sample some test statistics and plot their distribution:\n\n\n   ::: {.cell}\n   \n   ```{.r .cell-code}\n   n <- 10000\n   t_stat <- rt(n, df = 10)\n   hist(t_stat)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](001_pvalue-distribution_files/figure-html/unnamed-chunk-1-1.png){width=672}\n   :::\n   :::\n\n\n2. For a left-sided t-test with t-distributed test statistic, the p-value is computed with `pt` and we can clearly see, that these p-values are uniformly distributed.\n\n\n   ::: {.cell}\n   \n   ```{.r .cell-code}\n   p_value <- pt(t_stat, df = 10)\n   hist(p_value)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](001_pvalue-distribution_files/figure-html/unnamed-chunk-2-1.png){width=672}\n   :::\n   :::\n\n\n## Build some intuition where the uniform distribution of the p-value is coming from\n\nTo build intuition on why the p-value is uniformly distributed under the null hypothesis, have a look at the following plots ^[I have used `df = 1` here for pedagogical reasons.]:\n\n\n::: {.cell .preview-image}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(2, 1), mar = c(4, 4, 1, 1))\n\n# t-distribution with df = 1\n\ncurve(pt(x, df = 1), xlab = \"\", ylab = \"p-value\", ylim = c(0,1), xlim = c(-7, 7))\ny_values <- seq(0.05, 0.95, by = 0.05)\nx_values <- qt(y_values, df = 1)\nsegments(x_values, rep(-0.5, length(x_values)), x_values, y_values, col = \"red\", lty = 2)\nsegments(x_values, y_values, rep(-7.5, length(y_values)), y_values, col = \"blue\", lty = 2)\naxis(side = 1, col.axis = \"red\")\naxis(side = 2, col.axis = \"blue\")\n\ncurve(dt(x, df = 1), xlab = \"test statistic\", ylab = \"density\", xlim = c(-7, 7))\nabline(v = x_values, col = \"red\", lty = 2)\naxis(side = 1, col.axis = \"red\")\n```\n\n::: {.cell-output-display}\n![](001_pvalue-distribution_files/figure-html/fig-quantile-plot-1.png){#fig-quantile-plot width=672}\n:::\n:::\n\nPer definition, a left-sided p-value computes the ratio of test statistics that would fall below the currently observed value of the test statistic (always assuming $H_0$ is indeed true). On the top left side in @fig-quantile-plot we have divided the range of the p-value between 0 and 1 into equal <span style=\"color:blue;\">blue</span> intervals of length 0.05.\nPer definition, between two blue lines will fall 5% percent of values from the underlying test statistics.\nTo achieve this *equal* percentage everywhere, the distribution function in the upper plot has to \"collect\" test statistics from a larger <span style=\"color:red;\">red</span> interval on the bottom axis, when we move away from the mean.\nClose to the mean, we observe more values of the test statistic (as indicated by the density function in the bottom plot) so the interval that makes up 5% of the whole distribution will be small.\nIn other words, the p-value stretches the range of the test statistics into equal intervals.\nThis can best be seen with the largest red interval in the bottom left.\nBecause test statistics are so rarely observed so far away from the mean, we have to collect values from a very wide range to collect 5% of values for the test statistic.\n\n::: {.callout-note}\nThe technical reason for why the p-value is uniformly distributed under the null hypothesis, is the so-called [probability integral transform](https://en.wikipedia.org/wiki/Probability_integral_transform).\nFrom Wikipedia:\n\n>Suppose that a random variable $X$ has a continuous distribution for which the cumulative distribution function (CDF) is $F_X$. Then the random variable $Y$ defined as\n\n> $Y := F_X(X)$\n\n>has a standard uniform distribution.\n\nIn our case, $Y$ is the p-value and $X$ is the test statistic.\n:::\n\n\n## Imagine some animated version\n\nPerhaps at some point I will update the ugly plots and build a small animation:\n\n- Imagine how it would look like to continuously sample test statistics at the bottom of the lower plot. Most points (i.e., observed values of the test statistic) would appear close to the middle but, we would also get rare points further outside.\n- Imagine that each point would *fly upwards* within the <span style=\"color:red;\">red</span> corridor until it hits the curve of the distribution function in the upper plot and then *flies left* within the corresponding <span style=\"color:blue;\">blue</span> corridor.\n- Image that all incoming points are collected on the left side of the upper plot and are used to continuously update a histogram with the blue corridor as bars.\n\n",
    "supporting": [
      "001_pvalue-distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}