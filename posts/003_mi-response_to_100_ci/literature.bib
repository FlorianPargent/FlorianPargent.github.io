@article{rohrer_2025,
title = {Rethinking measurement invariance causally},
journal = {Current Research in Ecological and Social Psychology},
volume = {9},
pages = {100241},
year = {2025},
issn = {2666-6227},
doi = {https://doi.org/10.1016/j.cresp.2025.100241},
url = {https://www.sciencedirect.com/science/article/pii/S2666622725000280},
author = {Julia M. Rohrer and Borysław Paulewicz},
keywords = {Measurement invariance, Measurement, Causal inference, Structural equation modeling, Factor analysis},
abstract = {Measurement invariance is often touted as a necessary statistical prerequisite for group comparisons. Typically, when there is evidence against measurement invariance, the analysis ends. Here, we introduce readers to an alternative perspective on measurement invariance that shifts the focus from statistical procedures to causality. From that angle, violations of measurement invariance imply that there are potentially interesting differences in the measurement process between the groups, which could warrant explanations in their own right. We illustrate this with hypothetical examples of substantively meaningful violations of metric, scalar, and residual invariance. At the same time, standard procedures to test for measurement invariance rest on strong causal assumptions about the data-generating process that researchers may often be unwilling to endorse in other contexts. We point out two very different ways forward. First, for researchers who want to commit to latent factor models, violations of measurement invariance can be followed up with investigations into why those violations occur, turning them from a dead end into new research questions. Second, for researchers who feel more ambivalent about latent factor models, alternatives may be considered, and group differences on sum scores and item scores may be reported anyway as interesting descriptive findings—but they should be followed up with discussions of various explanations that take into account their plausibility.}
}

@article{sterner_2024,
author = {Philipp Sterner and Florian Pargent and Dominik Deffner and David Goretzko},
title = {A Causal Framework for the Comparability of Latent Variables},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
volume = {31},
number = {5},
pages = {747--758},
year = {2024},
publisher = {Routledge},
doi = {10.1080/10705511.2024.2339396},
URL = {https://doi.org/10.1080/10705511.2024.2339396
},
eprint = {https://doi.org/10.1080/10705511.2024.2339396
}
}

@article{cinelli_2024,
author = {Carlos Cinelli and Andrew Forney and Judea Pearl},
title ={A Crash Course in Good and Bad Controls},
journal = {Sociological Methods \& Research},
volume = {53},
number = {3},
pages = {1071-1104},
year = {2024},
doi = {10.1177/00491241221099552},
URL = {https://doi.org/10.1177/00491241221099552},
eprint = {https://doi.org/10.1177/00491241221099552},
abstract = { Many students of statistics and econometrics express frustration with the way a problem known as “bad control” is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this “crash course” accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models. }
}